{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anhbu\\miniconda3\\envs\\odl\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as it\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from preprocess import VietnameseTextCleaner, dict_handler\n",
    "from utils import load_json\n",
    "\n",
    "matplotlib.style.use(\"ggplot\")\n",
    "\n",
    "DATA_DIR = \"C:\\\\Users\\\\anhbu\\\\Desktop\\\\new_odl\\\\data\\\\fevent\"\n",
    "CUR_DIR = os.path.abspath(os.curdir)\n",
    "CACHE_DIR = \"E:\\\\tools\\\\new_odl\\\\cache\"\n",
    "VNCORE_NLP_PATH = os.path.join(CUR_DIR, \"../vncorenlp/\")\n",
    "STOPWORDS_PATH = os.path.join(\n",
    "    CUR_DIR, \"../stop_words/vietnamese-stopwords-dash.txt\"\n",
    ")\n",
    "os.environ[\"JAVA_HOME\"] = os.path.join(os.environ[\"CONDA_PREFIX\"], \"Library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = VietnameseTextCleaner(\n",
    "    stopwords_path=STOPWORDS_PATH,\n",
    "    vncorenlp_path=VNCORE_NLP_PATH,\n",
    "    cur_dir=CUR_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for file in os.listdir(DATA_DIR):\n",
    "    small_data = load_json(DATA_DIR, file)\n",
    "    if file == \"CritiCat.envent.json\":\n",
    "        for post in small_data:\n",
    "            if post[\"event\"] == \"Unknown event\":\n",
    "                post[\"event\"] = \"huflit\"\n",
    "    data.extend(small_data)\n",
    "\n",
    "for post in data:\n",
    "    if \"event\" not in post:\n",
    "        post[\"event\"] = \"Unknown event\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_usefull_data(post):\n",
    "    data = {}\n",
    "\n",
    "    data[\"id\"] = post[\"_id\"][\"$oid\"]\n",
    "    data[\"event\"] = post[\"event\"]\n",
    "    data[\"label\"] = post[\"label\"]\n",
    "    data[\"post_message\"] = post[\"data\"][\"post_message\"]\n",
    "    data[\"num_like_post\"] = post[\"data\"][\"num_like_post\"]\n",
    "    data[\"num_comment_post\"] = post[\"data\"][\"num_comment_post\"]\n",
    "    data[\"num_share_post\"] = post[\"data\"][\"num_share_post\"]\n",
    "    data[\"image\"] = post[\"data\"][\"images\"]\n",
    "\n",
    "    try:\n",
    "        # print(\n",
    "        #     list(post[\"data\"][\"metadata\"][\"page_insights\"].values())[\n",
    "        #         \"publish_time\"\n",
    "        #     ]\n",
    "        # )\n",
    "        data[\"user_name\"] = list(\n",
    "            post[\"data\"][\"metadata\"][\"page_insights\"].values()\n",
    "        )[0][\"page_id\"]\n",
    "        data[\"timestamp_post\"] = list(\n",
    "            post[\"data\"][\"metadata\"][\"page_insights\"].values()\n",
    "        )[0][\"post_context\"][\"publish_time\"]\n",
    "    except:\n",
    "        # print(post[\"data\"][\"metadata\"][\"content_owner_id_new\"])\n",
    "        data[\"user_name\"] = post[\"data\"][\"metadata\"][\"content_owner_id_new\"]\n",
    "        data[\"timestamp_post\"] = None\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "clean_data = [get_usefull_data(post) for post in data]\n",
    "\n",
    "df = pd.DataFrame(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1468 entries, 0 to 1467\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   id                1468 non-null   object \n",
      " 1   event             1468 non-null   object \n",
      " 2   label             1468 non-null   int64  \n",
      " 3   post_message      1468 non-null   object \n",
      " 4   num_like_post     1468 non-null   object \n",
      " 5   num_comment_post  1468 non-null   object \n",
      " 6   num_share_post    1468 non-null   object \n",
      " 7   image             1468 non-null   object \n",
      " 8   user_name         1468 non-null   object \n",
      " 9   timestamp_post    1249 non-null   float64\n",
      "dtypes: float64(1), int64(1), object(8)\n",
      "memory usage: 114.8+ KB\n"
     ]
    }
   ],
   "source": [
    "mean = df[\"timestamp_post\"].mean()\n",
    "df.timestamp_post.fillna(mean)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numerical_string(string):\n",
    "    pattern = r\"\\d+,*\\d+[Kk]*\"\n",
    "    match = re.match(pattern, str(string))\n",
    "\n",
    "    if match:\n",
    "        number = match.group(0)\n",
    "        mul = 1\n",
    "        if number[-1].lower() == \"k\":\n",
    "            number = number[:-1]\n",
    "            mul = 1000\n",
    "        number = number.replace(\",\", \".\")\n",
    "        return int(float(number) * mul)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "df.num_share_post = df.num_share_post.apply(convert_numerical_string)\n",
    "df.num_like_post = df.num_like_post.apply(convert_numerical_string)\n",
    "df.num_comment_post = df.num_comment_post.apply(convert_numerical_string)\n",
    "df.post_message = df.post_message.apply(cleaner.clean_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values(\"timestamp_post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown event\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [00:13<00:00, 35.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ba-phuong-hang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:02<00:00, 44.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hao-nam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [00:09<00:00, 26.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huflit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129/129 [00:03<00:00, 39.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nu sinh tu tu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 39.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shark-binh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [00:07<00:00, 32.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tranthanh-cgv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:04<00:00, 32.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "viec-lam-online\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:02<00:00, 42.33it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ImageTransform(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageTransform, self).__init__()\n",
    "        self.toTensor = it.ToTensor()\n",
    "        self.resize = it.Resize((256, 256), antialias=True)\n",
    "\n",
    "    def forward(self, images):\n",
    "        images = self.toTensor(images)\n",
    "        images = self.resize(images)\n",
    "        return images / 255\n",
    "\n",
    "\n",
    "image_transform = ImageTransform()\n",
    "\n",
    "\n",
    "def add_image(dataset):\n",
    "    for data in tqdm(dataset):\n",
    "        if len(data[\"image\"]) != 0:\n",
    "            try:\n",
    "                image = Image.open(\n",
    "                    requests.get(data[\"image\"][0], stream=True).raw\n",
    "                )\n",
    "                temp = image_transform(\n",
    "                    np.array(image.convert(\"RGB\")).astype(np.float32)\n",
    "                )\n",
    "            except:\n",
    "                temp = (\n",
    "                    torch.randint(0, 256, (3, 256, 256), dtype=torch.float)\n",
    "                    / 255\n",
    "                )\n",
    "        else:\n",
    "            temp = torch.randint(0, 256, (3, 256, 256), dtype=torch.float) / 255\n",
    "\n",
    "        data[\"image\"] = temp\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for name, group in df_sorted.groupby(\"event\"):\n",
    "    print(name)\n",
    "    dataset.append(add_image(group.to_dict(\"records\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved clean_metadata_with_image_01.pt with size 356 MB\n",
      "Saved clean_metadata_with_image_02.pt with size 76 MB\n",
      "Saved clean_metadata_with_image_03.pt with size 181 MB\n",
      "Saved clean_metadata_with_image_04.pt with size 97 MB\n",
      "Saved clean_metadata_with_image_05.pt with size 20 MB\n",
      "Saved clean_metadata_with_image_06.pt with size 189 MB\n",
      "Saved clean_metadata_with_image_07.pt with size 116 MB\n",
      "Saved clean_metadata_with_image_08.pt with size 68 MB\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "save_dir = os.path.join(CACHE_DIR, \"clean_metadata_with_image/fevent\")\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "for i, chunk in enumerate(dataset):\n",
    "    filename = f\"clean_metadata_with_image_{i+1:02}.pt\"\n",
    "    file_path = os.path.join(save_dir, filename)\n",
    "    torch.save(copy.deepcopy(chunk), file_path)  # Save\n",
    "    print(\n",
    "        f\"Saved {filename} with size {(os.path.getsize(file_path)/1024**2):.0f} MB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_range_by_weeks(start_timestamp, end_timestamp):\n",
    "#     start_date = datetime.datetime.fromtimestamp(start_timestamp).date()\n",
    "#     end_date = datetime.datetime.fromtimestamp(end_timestamp).date()\n",
    "#     week_start = start_date - datetime.timedelta(days=start_date.weekday())\n",
    "#     week_end = week_start + datetime.timedelta(days=6)\n",
    "#     while week_end < end_date:\n",
    "#         yield (\n",
    "#             int(\n",
    "#                 datetime.datetime.combine(\n",
    "#                     week_start, datetime.datetime.min.time()\n",
    "#                 ).timestamp()\n",
    "#             ),\n",
    "#             int(\n",
    "#                 datetime.datetime.combine(\n",
    "#                     week_end, datetime.datetime.max.time()\n",
    "#                 ).timestamp()\n",
    "#             ),\n",
    "#         )\n",
    "#         week_start = week_end + datetime.timedelta(days=1)\n",
    "#         week_end = week_end + datetime.timedelta(days=7)\n",
    "#     yield (\n",
    "#         int(\n",
    "#             datetime.datetime.combine(\n",
    "#                 week_start, datetime.datetime.min.time()\n",
    "#             ).timestamp()\n",
    "#         ),\n",
    "#         int(datetime.datetime.fromtimestamp(end_timestamp).timestamp()),\n",
    "#     )\n",
    "\n",
    "\n",
    "# def timestamp_to_id(timestamp, ranges):\n",
    "#     return int(\n",
    "#         datetime.datetime.fromtimestamp(timestamp).strftime(\"%Y%m%d%H%M%S\")\n",
    "#     )\n",
    "\n",
    "\n",
    "# begin = datetime.datetime(2023, 1, 1).timestamp()\n",
    "# end = datetime.datetime(2023, 6, 8).timestamp()\n",
    "# wrs = []\n",
    "# for week_range in split_range_by_weeks(begin, end):\n",
    "#     wrs.append(week_range)\n",
    "\n",
    "# wrs = [(k, v) for k, v in enumerate(wrs)]\n",
    "\n",
    "# df = pd.DataFrame(clean_data)\n",
    "\n",
    "# df = df[[\"timestamp_post\", \"event\"]]\n",
    "\n",
    "\n",
    "# def get_week(timestamp):\n",
    "#     for k, v in wrs:\n",
    "#         if v[0] <= timestamp <= v[1]:\n",
    "#             return k\n",
    "#     return -1\n",
    "\n",
    "\n",
    "# df[\"timestamp_post\"] = df[\"timestamp_post\"].apply(get_week)\n",
    "\n",
    "# df = pd.get_dummies(df, columns=[\"event\"])\n",
    "\n",
    "# df.groupby(\"timestamp_post\").sum().plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
